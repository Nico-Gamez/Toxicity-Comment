# Comment_Toxicity
Project created to allow an **input to be analyzed and define its level of toxicity.**

If you want to test it, run the first imports, the df variable, the imports of the pre-process to vectorizer.adapt variable, adn the model from the Title "Test and Gradio", loading the file "toxicity.h5".

![First imports](https://user-images.githubusercontent.com/99749668/167062283-a49879ce-aab9-4d93-9742-6428209832fd.png)

![Second imports](https://user-images.githubusercontent.com/99749668/167062345-f7f7eb58-1f7b-406e-b2da-11743d671adc.png)


![toxicity load](https://user-images.githubusercontent.com/99749668/167028099-3d9806c5-6276-4330-a4bd-77a3ad750aa2.png)


If you want, you can try it from the **Gradio interface**.

![Example](https://user-images.githubusercontent.com/99749668/167062017-9d5ee2bd-a508-4c68-8fd9-10c7190e99ed.png)



**Important!**

Due to the components of my pc I couldn't train it much :(
But it is recommended if you are going to try running it more than 10 times, in order to improve the results.  It is in the following line of code

![epochs](https://user-images.githubusercontent.com/99749668/167028124-d3ae4861-368e-4d16-882a-cb95b25ee17d.png)


Hope you like.
